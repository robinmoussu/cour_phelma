<!--
# AUTHOR:   robin moussu
# FILE:     cour.mkd
# ROLE:     Cour de probabilités et statistiques
# CREATED:  2014-03-24 13:34:50
# MODIFIED: 2014-03-31 16:50:20
# CONTACT:  eric.moisan@gipsa-lab.grenoble-inp.fr
-->

\newcommand \differential[2] {
    \frac{\mathrm{d} #1}{\mathrm{d}#2}
}

Probabilités et statistiques
============================


## INTRODUCTION ##

Au 17ème siècle PASCAL et DE FERMAT ont basé leur travaux sur la Théorie des jeux.
=> À donné naissance à toutes la théorie combinatoire.

Au 20ème siècle, KOLMOGOROV à formalisé les statistiques pour arriver au formalisme moderne. Ses travaux sont basés sur la théorie de la mesure).

### Notion intuitive de probabilité ###

+ Sur 1D6, la probabilité d'obtinir un 5 est de $\frac{1}{6}$ uniquement si le dé est équilibré.

+ Soit N lancés de 1D6 équilibré.
La probabilités d'obtenir un 5 n'est valable que pour N grand.

## MESURE ET PROBABILITÉS ##

### Vocabulaire ###

Une Épreuve aléatoire
$\Omega =$ {résultats possibles}   
$\omega_p =$ résultats élémentaires

### exemple du dé ###

$\omega_1 = 1, \omega_2 =2$   
$\Omega = \{ 1, 2, 3, 4, 5, 6 \}$   
$A \subset \Omega$ évènements = résultats possibles

$A$ = famille de $A_i$ vérifiant 3 règles :

- $\Omega \in A$
- $\forall A_i \in A, C_\Omega ^{A_i} \in A$
- $\forall A_i \in A, i \in A_i \in A$

L'ensemble forme une tribut $A$

### Définition ###
On associe à chaque $A_i \in A$ un nombre $\equiv$ probabilité
\begin{displaymath}
\begin{array}{r l}
p: & A \rightarrow [O,1] \\
   & A_i \rightarrow p(Ax) = \text{probabilité de réaliser} A_i \text{sur une épreuve.} \\
\end{array}
\end{displaymath}

P vérifie 3 règles:

$$A_i \exists A, p(\emptyset) = 0 \leq p(A_i) \leq 1 = p(\Omega)$$
$$A_i \exists A, p(C_\Omega A_i) = 1 - p(A_i)$$
$$A_i \exists A, i \in I p(U(i\in I) A_i)  = \sum_{i\in I} p(A_i)$$

ssi les $A_i$ sont 2 à 2 disjoints.

### exemple ###

dé à 6 faces
\begin{displaymath}
\begin{array}{r l}
\Omega & = \{1,2,3,4,5,6\} \\
A_1 & = \{\Omega, \emptyset\} \\
A_2 & = \{\Omega, \emptyset, \{1,3,5\},\{2,4,6\}\} \\
A_3 & = \{\Omega, \emptyset, \{1\}, \{2\}, \{2,3,4,5,6\}, \{1,3,4,5,6\}, \{1,2\}, \{1,4,5,6\}\} \\
(\Omega, A, p) & \equiv \text{espace probabilisé}
\end{array}
\end{displaymath}


\begin{tabular}{r l}
$A_i$ et $A_j$ incompatible & $p(A_i \cap A_j) = 0$ \\
$A_i$ et $A_j$ indépendants & $p(A_i \cap A_j) = p(A_i)\times p(A_j)$ \\
Probabilité inconditionnelle & $p(A_i \mid A_j) = \frac{p(A_i \cap A_j)}{p(A_j)}$ \\
\end{tabular}

\begin{tabular}{r l}
proba & $A_i$ conditionnelement à $A_j$ \\
      & $A_i$ sachant $A_j$
\end{tabular}

### Loi des probabilités totales ###
partition $(A_i)$ de $\omega \in A \rightarrow \cup_i A_i = \Omega, \forall i \neq j, A_i \cap A_j = \emptyset$

$\forall B \in A, p(B) = \sum\limits_{i\in I} p(B\mid A_i) p(A_i)$

### Formules de Bayes ###

Partition $(A_i)$ de $\Omega$ :
$\forall B \in A$   

\begin{displaymath}
\begin{array}{r l}
p(A_k \mid B) &= \frac{p(B \mid A_k) p(A_k)}{\sum_i p(B \mid A_i ) p(A_i)} \\
              &= \frac{p(A_k \cap B)}{p(B)} \\
\end{array}
\end{displaymath}

$B = \cup_i (B \cap A_i) \equiv$ partition de B.   
$\rightarrow p(B) = \sum_i p(B \cap A_i) = \sum_i p(A \mid A_i) p(A_i)$

### Propriété de la mesure de proba ###

$\forall A_i$ et $A_j$ disjoints $p(A_i \cap A_j) = p(A_i) + p(A_j)$   
Dans le cas général :   
$p(A_i \cap A_j) = p(A_i) + p(A_j) -p(A_i \cap A_j)$

**Voir schéma 1**

### Exercice ###

Je cherche mon cours, que j'ai rangé dans mon bureau avec une probabilité p. Le bureau possède 3 tiroirs semblables.

- J'ouvre le 1er tiroir … en vain
- j'ouvre le 2ème tiroir … en vain

Quelle est la probabilité de le trouver dans le 3 ème tiroir ?

Soit $A$ le cour dans le 3ème tiroir.   
Soit $B$ le cour ni dans le 1er, ni dans le 2ème.

\begin{displaymath}
\begin{array}{r l}
p(A \mid B) &= \frac{p(A \cap B)}{p(B)} \\
            &= \frac{p(A)}{p(B)} \text{ car A est compris dans B}
\end{array}
\end{displaymath}

Au départ : $\Omega = \{\omega_1, \omega_2, \omega_3, \omega_0\}$   
avec $\omega_0 =$ pas dans le bureau.   
$\omega_i =$ dans le tiroir $i (i = 1,2,3)$

La tribu utile sur cet exemple :
\begin{displaymath}
\begin{array}{r l}
A_1 &= \{\Omega, \emptyset, \{\omega_1\}, \{\omega_2\}, \{\omega_1,\omega_2\}, \{\omega_2,\omega_3,\omega_0\},…\} \\
A_2 &= \{\Omega, \emptyset, \{\omega_1,\omega_2\}, \{\omega_3,\omega_0\},\{\omega_0\}, \{\omega_3\},\{\omega_1,\omega_2,\omega_3\},\{\omega_0,\omega_1,\omega_2\}\} \\
\end{array}
\end{displaymath}

Pour $A_2$, les probabilités sont respectivement de $\{1,0,\frac{2\cdot}{3},1-p+\frac{p}{3},1-p,\frac{p}{3},p,1-p+\frac{2\cdot p}{3}\}$
\begin{displaymath}
A_2' = \{\{\omega_3,\omega_0\},\emptyset,\{\omega_3\},\{\omega_0\}\}
\end{displaymath}

## VARIABLE ALEATOIRE RÉELLE CONTINUE ##

Soit une variable aléatoir x à valeurs sur $\mathbb{R}$

$$ \Omega = \mathbb{R} $$
$$ A_i \subset \Omega $$
$$ A = \{A_i\} = tribu $$

Les $A_i$ de base sont $]-\infty ; x ]$
$\hookrightarrow$ tribu Borclienne $\equiv \mathcal{B}_{\mathbb{R}}$

### Fonction de répartition de la variable aléatoire x ###

\begin{displaymath}
\begin{array}{rl}
F_x : &\mathbb{R} \rightarrow [0,1] \\
&x \in \mathbb{R} \rightarrow \text{proba} (x \in ]-\infty, x]) \\
\end{array}
\end{displaymath}

Il existe 3 types de variables aleatoires.

\begin{tabular}{l l}
- continue : &$F_x$ est continue. **cf fig 2** \\
- discrete : &$F_x$ est continue par morceaux. **cf fig 3** \\
- mixte : &(ex: temps d'attente à un feu) **cf fig 3** \\
\end{tabular}

propritétés de $F_x$ :
$$\forall x \in \mathbb{R}$$
$$F_x (-\infty) \leq F_x (x) \leq 1 = F_x (+\infty)$$
$$F_x \text{ est croissante}$$
$$\forall a, b \in \mathbb{R}, a \leq b$$
$$F_x(b) - F_x()$$
$$]-\infty, b] \  ]-\infty, a]$$

$$F_x(x) = \int\limits_{-\infty}^{\infty} f_x (u) \mathrm{d}$$

fct répartition
$$f_x (x) = \differential{F_x}{x} \text{ (au sens des distributions)}$$

Densité de probabilité $f_x = \differential{F_x}{x} \leq 0$i.   
$f_x$ est sommable : $\int_{\mathbb{R}} f_x (x) \mathrm{x} = 1 \rightarrow \hat{f_x}$ existe (le chapeau correspond à la transformée de Fourier).

\begin{tabular} {ll}
La probabilité de $(x \in ]a,b)$ &= $F_x(b) - F_x(a)$ \\
&= $\int\limits_a^b F_x(x) \mathrm{d}x$ \\
\end{tabular}

### exemple ###

Loi uniforme sur [a,b], avec b > a.

$f_x$ est constante sur [a,b], et nulle ailleurs

**cf ex 5**

### Loi de Gauss ###

C'est la même chose que la loi normale $\mathrm{N} (m,\sigma^2).
$$F_x(x) = \frac{1}{\sigma \sqrt{2\pi}} \exp{-\frac{x - m)^2}{2\sigma^2}}$$

**cf fig 6**

NB : $F_x(x) \rightarrow$ table de loi

### Loi de Cauchy (a>0) ###

$$f_x(x) = \frac{1}{\pi} \frac{a}{a^2 + x^2}$$
$$F_x(x) = \frac{1}{2} + \frac{1}{\pi}\arctan{\frac{x}{a}}$$

### Espérance mathématique d'une variable aléatoire x de densité de probabilité $f_x$ ###

\boxed{E\{x\} = \int\limits_{\mathbb{R}} x f_x(x) \mathrm{d}}

NB : peut ne pas exister.

exemple : Cauchy $E(x)= +\infty$
$$E\{g(x)\} = \int\limits_{\mathbb{R} g(x)f_x(x) \mathrm{d}}$$

La fonction g doit être mesurable $\sim$ continue par morceaux, en particulier : $g(x) = x^n$
$$E(x^n) = \int_{\mathbb{R}} x^n f_x(x) \mathrm{d}x$$
On appelle cette fonction un moments d'ordre n.

\begin{tabular}{l l}
n = 1 & $\rightarrow \text{moyenne de } x$ \\
n = 2 & $\rightarrow \text{dispertion de } x$ \\
\end{tabular}

Variance de x : $E\{[x-E(x)]^2\} = \text{var(x)}$ (NB: la partie entre crochet est une variable aléatoire x contrée).

\begin{tabular}{r l}
$\text{var}(x)$ &= $E(x^2) -E^2(x)$ \\
&= $E\{x^2 - 2xE(x)+E^2(x)\}$ \\
&= $E (x^2) - 2E(x)E(x) + E^2(x)$ \\
&= $E (x^2) - E^2(x)$ \\
\end{tabular}

NB : $E$ est un opérateur binaire.
\begin{tabular}{l l}
E(x) &= $\int_\mathbb{R} x F_x(x) \mathrm{d}x$ \\
E(ax + b) &= $\int_\mathbb{R} (ax+b)f_x(x)\mathrm{d}x$ \\
&= $a\int_\mathbb{R} x f_x(x) \mathrm{d}x +b\int_\mathbb{R} x f_x(x) \mathrm{d}x$ \\
\end{tabular}

### Inégalité de Bienaginé - Chebychev ###

Variable aléatoire x de densité de probabilité $f_x$.
La probabilité $(\lvert x - E(x)\rvert > a) \leq \frac{\sigma^2}{a^2}$

\begin{tabular}{l l}
$\sigma^2$ & $\equiv \text{var}(x)$ \\
$\sigma$ & $\equiv \sqrt{\text{var}(x)}$ \\
\end{tabular}

$\sigma$ est l'écart type.

**Cf démo dans le poly**

### Changement de variable ###

v.a.x de la loi connue $f_x$.

- Loi bijection croissante :

$$y = h(x)$$
$$h \equiv \text{continue par morceaux}$$

**cf fig 7**

\begin{tabular}{r l}
$F_y(y)$ &= $p(y \leq y)$ \\
&= $p(x \leq h^{-1}(y))$ \\
&= $F_x(h^{-1}(y))$ \\
$F_y(y)$ &= $\differential{}{y} F_y(y)$ \\
&= $\differential{}{x} F_x(h^{-1}(y)) \differential{}{y}h^{-1}(y)$ \\
&= $f_x(h^{-1}(y))\differential{x}{y}$ \\
\end{tabular}

- Loi bijection décroissante :

**cf fig 8**

\begin{tabular}{r l}
$F_y(y)$ &= $p(x \in [h^{-1}(y), +\infty[)$ \\
&= $1 - F_x (h^{-1}(y))$ \\
$f_y(y)$ &= $-f_x(h^{-1}(y)) \differential{h^{-1}(y)}{y}$ \\
\end{tabular}

- Loi bijective :

$$f_y(y) = f_x(h^{-1} (y)) | \differential{x}{y} |$$

- Cas général :

soient $\{x_i\}, i \in I$ les antécédants de y.
$$h(x_i) = y \forall i \in I$$
$$f_y(y) = \sum\limits_{i\in I} f_x(h^{-1}(y)) | \differential{x}{y} |$$

### exercice : x uniforme sur $[-\frac{\pi}{2}, \frac{3\pi}{2}[$ ###

$y = a \sin{x}, a > 0$

**cf ex 9**

$$\forall y \text{ tel que } |y| > a, f_y(y) = 0$$
$$f_y(y) = f_x(x_i) |\differential{x}{y}|_{x = x_i} + f_x(x_i) |\differential{x}{y}|_{x = x_2} = \frac{1}{\pi} \frac{\frac{1}{a}}{\sqrt{1-\frac{y^2}{a^2}}}$$

$$x_1 = \arcsin(\frac{y}{a})$$
$$\Rightarrow \differential{x}{y} = \frac{\frac{1}{a}}{\sqrt{1-(\frac{y}{a})^2}}$$
$$ x_2 = \pi - x_1$$
$$ x_2 = \pi - \arcsin(\frac{y}{a})$$
$$\Rightarrow |\differential{x}{y}|_{x = x_2} = |\differential{x}{y}|_{x = x_2} = \frac{\frac{1}{a}}{\sqrt{1-\frac{y^2}{a^2}}}$$





## DISCRETE ##

## VECTORIELLE ##

## REGRESSION ##

## THEOREMES AUX LIMITES ##

## ESTIMATION PARAMÈTREQUE ##

## TESTS D'HYPETHÉSES ##

## Références ##

BASS : Éléments de calcul des probabilités MASSON
VENTETSEL : Théorie des probabilités MIR
RÉNYI : Calcul des probabilités MASSON (I.GABAY)
JAFFARD : Méthodes de la statistique MASSON
Série SCHAUM
chamilo


