<!--
# AUTHOR:   robin moussu
# FILE:     cour.mkd
# ROLE:     Cour de probabilités et statistiques
# CREATED:  2014-03-24 13:34:50
# MODIFIED: 2014-04-11 12:36:53
# CONTACT:  eric.moisan@gipsa-lab.grenoble-inp.fr
-->

\newcommand \differential[2] {
    \frac{\mathrm{d} #1}{\mathrm{d}#2}
}
\newcommand \dd[1] {
    \mathrm{#1}
}

Probabilités et statistiques
============================


## INTRODUCTION ##

au 17ème siècle PASCAL et DE FERMAT ont basé leur travaux sur la Théorie des jeux.
=> À donné naissance à toutes la théorie combinatoire.

Au 20ème siècle, KOLMOGOROV à formalisé les statistiques pour arriver au formalisme moderne. Ses travaux sont basés sur la théorie de la mesure).

### Notion intuitive de probabilité ###

+ Sur 1D6, la probabilité d'obtinir un 5 est de $\frac{1}{6}$ uniquement si le dé est équilibré.

+ Soit N lancés de 1D6 équilibré.
La probabilités d'obtenir un 5 n'est valable que pour N grand.

## MESURE ET PROBABILITÉS ##

### Vocabulaire ###

Une Épreuve aléatoire
$\Omega =$ {résultats possibles}   
$\omega_p =$ résultats élémentaires

### exemple du dé ###

$\omega_1 = 1, \omega_2 =2$   
$\Omega = \{ 1, 2, 3, 4, 5, 6 \}$   
$A \subset \Omega$ évènements = résultats possibles

$A$ = famille de $A_i$ vérifiant 3 règles :

- $\Omega \in A$
- $\forall A_i \in A, C_\Omega ^{A_i} \in A$
- $\forall A_i \in A, i \in A_i \in A$

L'ensemble forme une tribut $A$

### Définition ###
On associe à chaque $A_i \in A$ un nombre $\equiv$ probabilité
\begin{displaymath}
\begin{array}{r l}
p: & A \rightarrow [O,1] \\
   & A_i \rightarrow p(Ax) = \text{probabilité de réaliser} A_i \text{sur une épreuve.} \\
\end{array}
\end{displaymath}

P vérifie 3 règles:

$$A_i \exists A, p(\emptyset) = 0 \leq p(A_i) \leq 1 = p(\Omega)$$
$$A_i \exists A, p(C_\Omega A_i) = 1 - p(A_i)$$
$$A_i \exists A, i \in I p(U(i\in I) A_i)  = \sum_{i\in I} p(A_i)$$

ssi les $A_i$ sont 2 à 2 disjoints.

### exemple ###

dé à 6 faces
\begin{displaymath}
\begin{array}{r l}
\Omega & = \{1,2,3,4,5,6\} \\
A_1 & = \{\Omega, \emptyset\} \\
A_2 & = \{\Omega, \emptyset, \{1,3,5\},\{2,4,6\}\} \\
A_3 & = \{\Omega, \emptyset, \{1\}, \{2\}, \{2,3,4,5,6\}, \{1,3,4,5,6\}, \{1,2\}, \{1,4,5,6\}\} \\
(\Omega, A, p) & \equiv \text{espace probabilisé}
\end{array}
\end{displaymath}


\begin{tabular}{r l}
$A_i$ et $A_j$ incompatible & $p(A_i \cap A_j) = 0$ \\
$A_i$ et $A_j$ indépendants & $p(A_i \cap A_j) = p(A_i)\times p(A_j)$ \\
Probabilité inconditionnelle & $p(A_i \mid A_j) = \frac{p(A_i \cap A_j)}{p(A_j)}$ \\
\end{tabular}

\begin{tabular}{r l}
proba & $A_i$ conditionnelement à $A_j$ \\
      & $A_i$ sachant $A_j$
\end{tabular}

### Loi des probabilités totales ###
partition $(A_i)$ de $\omega \in A \rightarrow \cup_i A_i = \Omega, \forall i \neq j, A_i \cap A_j = \emptyset$

$\forall B \in A, p(B) = \sum\limits_{i\in I} p(B\mid A_i) p(A_i)$

### Formules de Bayes ###

Partition $(A_i)$ de $\Omega$ :
$\forall B \in A$   

\begin{displaymath}
\begin{array}{r l}
p(A_k \mid B) &= \frac{p(B \mid A_k) p(A_k)}{\sum_i p(B \mid A_i ) p(A_i)} \\
              &= \frac{p(A_k \cap B)}{p(B)} \\
\end{array}
\end{displaymath}

$B = \cup_i (B \cap A_i) \equiv$ partition de B.   
$\rightarrow p(B) = \sum_i p(B \cap A_i) = \sum_i p(A \mid A_i) p(A_i)$

### Propriété de la mesure de proba ###

$\forall A_i$ et $A_j$ disjoints $p(A_i \cap A_j) = p(A_i) + p(A_j)$   
Dans le cas général :   
$p(A_i \cap A_j) = p(A_i) + p(A_j) -p(A_i \cap A_j)$

**Voir schéma 1**

### Exercice ###

Je cherche mon cours, que j'ai rangé dans mon bureau avec une probabilité p. Le bureau possède 3 tiroirs semblables.

- J'ouvre le 1er tiroir … en vain
- j'ouvre le 2ème tiroir … en vain

Quelle est la probabilité de le trouver dans le 3 ème tiroir ?

Soit $A$ le cour dans le 3ème tiroir.   
Soit $B$ le cour ni dans le 1er, ni dans le 2ème.

\begin{displaymath}
\begin{array}{r l}
p(A \mid B) &= \frac{p(A \cap B)}{p(B)} \\
            &= \frac{p(A)}{p(B)} \text{ car A est compris dans B}
\end{array}
\end{displaymath}

Au départ : $\Omega = \{\omega_1, \omega_2, \omega_3, \omega_0\}$   
avec $\omega_0 =$ pas dans le bureau.   
$\omega_i =$ dans le tiroir $i (i = 1,2,3)$

La tribu utile sur cet exemple :
\begin{displaymath}
\begin{array}{r l}
A_1 &= \{\Omega, \emptyset, \{\omega_1\}, \{\omega_2\}, \{\omega_1,\omega_2\}, \{\omega_2,\omega_3,\omega_0\},…\} \\
A_2 &= \{\Omega, \emptyset, \{\omega_1,\omega_2\}, \{\omega_3,\omega_0\},\{\omega_0\}, \{\omega_3\},\{\omega_1,\omega_2,\omega_3\},\{\omega_0,\omega_1,\omega_2\}\} \\
\end{array}
\end{displaymath}

Pour $A_2$, les probabilités sont respectivement de $\{1,0,\frac{2\cdot}{3},1-p+\frac{p}{3},1-p,\frac{p}{3},p,1-p+\frac{2\cdot p}{3}\}$
\begin{displaymath}
A_2' = \{\{\omega_3,\omega_0\},\emptyset,\{\omega_3\},\{\omega_0\}\}
\end{displaymath}

## VARIABLE ALEATOIRE RÉELLE CONTINUE ##

Soit une variable aléatoir x à valeurs sur $\mathbb{R}$

$$ \Omega = \mathbb{R} $$
$$ A_i \subset \Omega $$
$$ A = \{A_i\} = tribu $$

Les $A_i$ de base sont $]-\infty ; x ]$
$\hookrightarrow$ tribu Borclienne $\equiv \mathcal{B}_{\mathbb{R}}$

### Fonction de répartition de la variable aléatoire x ###

\begin{displaymath}
\begin{array}{rl}
F_x : &\mathbb{R} \rightarrow [0,1] \\
&x \in \mathbb{R} \rightarrow \text{proba} (x \in ]-\infty, x]) \\
\end{array}
\end{displaymath}

Il existe 3 types de variables aleatoires.

\begin{tabular}{l l}
- continue : &$F_x$ est continue. **cf fig 2** \\
- discrete : &$F_x$ est continue par morceaux. **cf fig 3** \\
- mixte : &(ex: temps d'attente à un feu) **cf fig 3** \\
\end{tabular}

propritétés de $F_x$ :
$$\forall x \in \mathbb{R}$$
$$F_x (-\infty) \leq F_x (x) \leq 1 = F_x (+\infty)$$
$$F_x \text{ est croissante}$$
$$\forall a, b \in \mathbb{R}, a \leq b$$
$$F_x(b) - F_x()$$
$$]-\infty, b] \  ]-\infty, a]$$

$$F_x(x) = \int\limits_{-\infty}^{\infty} f_x (u) \mathrm{d}$$

fct répartition
$$f_x (x) = \differential{F_x}{x} \text{ (au sens des distributions)}$$

Densité de probabilité $f_x = \differential{F_x}{x} \leq 0$i.   
$f_x$ est sommable : $\int_{\mathbb{R}} f_x (x) \mathrm{x} = 1 \rightarrow \hat{f_x}$ existe (le chapeau correspond à la transformée de Fourier).

\begin{tabular} {ll}
La probabilité de $(x \in ]a,b)$ &= $F_x(b) - F_x(a)$ \\
&= $\int\limits_a^b F_x(x) \mathrm{d}x$ \\
\end{tabular}

### exemple ###

Loi uniforme sur [a,b], avec b > a.

$f_x$ est constante sur [a,b], et nulle ailleurs

**cf ex 5**

### Loi de Gauss ###

C'est la même chose que la loi normale $\mathrm{N} (m,\sigma^2).
$$F_x(x) = \frac{1}{\sigma \sqrt{2\pi}} \exp{-\frac{x - m)^2}{2\sigma^2}}$$

**cf fig 6**

NB : $F_x(x) \rightarrow$ table de loi

### Loi de Cauchy (a>0) ###

$$f_x(x) = \frac{1}{\pi} \frac{a}{a^2 + x^2}$$
$$F_x(x) = \frac{1}{2} + \frac{1}{\pi}\arctan{\frac{x}{a}}$$

### Espérance mathématique d'une variable aléatoire x de densité de probabilité $f_x$ ###

\boxed{E\{x\} = \int\limits_{\mathbb{R}} x f_x(x) \mathrm{d}}

NB : peut ne pas exister.

exemple : Cauchy $E(x)= +\infty$
$$E\{g(x)\} = \int\limits_{\mathbb{R} g(x)f_x(x) \mathrm{d}}$$

La fonction g doit être mesurable $\sim$ continue par morceaux, en particulier : $g(x) = x^n$
$$E(x^n) = \int_{\mathbb{R}} x^n f_x(x) \mathrm{d}x$$
On appelle cette fonction un moments d'ordre n.

\begin{tabular}{l l}
n = 1 & $\rightarrow \text{moyenne de } x$ \\
n = 2 & $\rightarrow \text{dispertion de } x$ \\
\end{tabular}

Variance de x : $E\{[x-E(x)]^2\} = \text{var(x)}$ (NB: la partie entre crochet est une variable aléatoire x contrée).

\begin{tabular}{r l}
$\text{var}(x)$ &= $E(x^2) -E^2(x)$ \\
&= $E\{x^2 - 2xE(x)+E^2(x)\}$ \\
&= $E (x^2) - 2E(x)E(x) + E^2(x)$ \\
&= $E (x^2) - E^2(x)$ \\
\end{tabular}

NB : $E$ est un opérateur binaire.
\begin{tabular}{l l}
E(x) &= $\int_\mathbb{R} x F_x(x) \mathrm{d}x$ \\
E(ax + b) &= $\int_\mathbb{R} (ax+b)f_x(x)\mathrm{d}x$ \\
&= $a\int_\mathbb{R} x f_x(x) \mathrm{d}x +b\int_\mathbb{R} x f_x(x) \mathrm{d}x$ \\
\end{tabular}

### Inégalité de Bienaginé - Chebychev ###

Variable aléatoire x de densité de probabilité $f_x$.
La probabilité $(\lvert x - E(x)\rvert > a) \leq \frac{\sigma^2}{a^2}$

\begin{tabular}{l l}
$\sigma^2$ & $\equiv \text{var}(x)$ \\
$\sigma$ & $\equiv \sqrt{\text{var}(x)}$ \\
\end{tabular}

$\sigma$ est l'écart type.

**Cf démo dans le poly**

### Changement de variable ###

v.a.x de la loi connue $f_x$.

- Loi bijection croissante :

$$y = h(x)$$
$$h \equiv \text{continue par morceaux}$$

**cf fig 7**

\begin{tabular}{r l}
$F_y(y)$ &= $p(y \leq y)$ \\
&= $p(x \leq h^{-1}(y))$ \\
&= $F_x(h^{-1}(y))$ \\
$F_y(y)$ &= $\differential{}{y} F_y(y)$ \\
&= $\differential{}{x} F_x(h^{-1}(y)) \differential{}{y}h^{-1}(y)$ \\
&= $f_x(h^{-1}(y))\differential{x}{y}$ \\
\end{tabular}

- Loi bijection décroissante :

**cf fig 8**

\begin{tabular}{r l}
$F_y(y)$ &= $p(x \in [h^{-1}(y), +\infty[)$ \\
&= $1 - F_x (h^{-1}(y))$ \\
$f_y(y)$ &= $-f_x(h^{-1}(y)) \differential{h^{-1}(y)}{y}$ \\
\end{tabular}

- Loi bijective :

$$f_y(y) = f_x(h^{-1} (y)) | \differential{x}{y} |$$

- Cas général :

soient $\{x_i\}, i \in I$ les antécédants de y.
$$h(x_i) = y \forall i \in I$$
$$f_y(y) = \sum\limits_{i\in I} f_x(h^{-1}(y)) | \differential{x}{y} |$$

### exercice : x uniforme sur $[-\frac{\pi}{2}, \frac{3\pi}{2}[$ ###

$y = a \sin{x}, a > 0$

**cf ex 9**

$$\forall y \text{ tel que } |y| > a, f_y(y) = 0$$
$$f_y(y) = f_x(x_i) |\differential{x}{y}|_{x = x_i} + f_x(x_i) |\differential{x}{y}|_{x = x_2} = \frac{1}{\pi} \frac{\frac{1}{a}}{\sqrt{1-\frac{y^2}{a^2}}}$$

$$x_1 = \arcsin(\frac{y}{a})$$
$$\Rightarrow \differential{x}{y} = \frac{\frac{1}{a}}{\sqrt{1-(\frac{y}{a})^2}}$$
$$ x_2 = \pi - x_1$$
$$ x_2 = \pi - \arcsin(\frac{y}{a})$$
$$\Rightarrow |\differential{x}{y}|_{x = x_2} = |\differential{x}{y}|_{x = x_2} = \frac{\frac{1}{a}}{\sqrt{1-\frac{y^2}{a^2}}}$$

### Rappels ###

$$F_x(x) = \text{proba} (X \leq x)$$
$$f_x(x) = \differential{F_x}{x} \text{d.d.p.}$$
$$E(g(x)) = \int_\mathrm{R} g(x) f_x(x) \dd x$$
$$f_x(x) \dd x = 1 et f_x (x) \geq 0$$
f_x chapeau (V), TF de f_x (x) existe###################

### Fct caractérisques ###
 

$$\phi_x : \mathrm{R} \rightarrow \mathrm{C}$$
$$ t \rightarrow E\{e^{itx}\}$$

<!--#############-->

$$\differential{k}{t^k} \phi_x(t) = \int\mathrm{R} (ix)^k e^{itx} f_x (x) \dd x$$
$$\phi_x ^{(k)} (0) = i^k \int\mathrm{R} x^k f_x(x) \dd x (\text{avec le contenue de l'intégrale = }E(x^k) )$$

Si la v.a.x possède des moments jusqu'à l'ordre k, sa fonction caractéristique est k fois dérivable en t=° et $\phi^{(k)}_x(0) = i^k E(x^k)$.

Ex : Loi gaussienne

\begin{displaymath}
\begin{array}{r l}
X \rightarrow \mathcal{N} (m, \sigma^2) \\
f_x(x) &= \frac{1}{\sigma\sqrt{2\pi}} \exp{-\frac{(x-m)^2}{2\sigma^2}} \\
\phi_x (t) &= e^{itm} e^{-t^2\sigma^2/2} \\
\end{array}
\end{displaymath}

$\infty$ dérivale en t=0.

$\rightarrow E(x^k)$ existe $\forall k$
$$E(x) = \int\mathrm{R} x f_x(x)\dd x$$
$$= ...$$
$$= m$$

$$\phi_x'(t) = ime^{itm}e^{-t^2\sigma^2/2}$$
$$= e^{itm} t\sigma^2 e^{-t^2\sigma^2/2}$$
$$\phi_x'(0) = \Im$$

**$Var(x) = E(x^2) - E^2(x)$**

Ex: Loi de Cauchy
$$f_x (x) = \frac{1}{\pi} \frac{a}{a^2 + x^2}$$
$$a \in \mathrm {R}$$
$$\forall k \in \mathrm {N^*} E(x^k = ... = \infty$$
$$\phi_x(t) = e^{-a|t|}$$

## V.A. RÉELLE DISCRETE ##
v.a.x d valeurs dans $\mathrm{R}$, mais :

\begin{align*}
\Omega &= \{x_1,x_2,...,x_n\} \\
&= \{x_i, i\in \mathrm{I} \\
\end{align*}

### Fct répartition ### 
 

$\forall x \in \mathrm{R} F_x(x) = \text{proba} (X \leq x)$

$F_x$ est continue à droite, continue par morceux, en tout x.
$$F_x(x_i) - \lim\limits{h\rightarrow 0^+} F_x (x-h) = proba (X = x_i)$$

### densité de probabilité ###
 

\begin{align*}
F_X(x) &= \differential{F_x}{x} (x) \\
f_X &= \differential{F_x}{x} \\
&= \sum\limits_{i \in I} \text{ proba } X = x_i \delta_{x_i} \\
\end{align*}

### Moments ###


$$E\{X^k\} = <f_x,x^k>$$
$$=\sum\limits_{i \in I} p(X = x_i) x_i^k$$

### Fonction Caractéristique ###
 

$$\phi_x(t) = \sum\limits_{l \in {I}} p(x=x_l) e^{itx_1}$$

### Exemple ###
 
- Loi de Bernouilli ($p\in [0,1]$). x peut valoir 1 ou 0.

ainsi :

\begin{displaymath}
\left \}
\begin{array}{r l}
\text{X = 1} &=p \\
\text{X = 0} &= 1 - p \\
\end{array}
\right .
\end{displaymath}

$p = 1/2$
Connu sous la loi de *pile ou face*.
$$\forall k \in \mathrm{N^*}, E(x^k) = p$$
$$\text{var}(x) = p - p^2$$
$$Var (x) = p(1-p)$$
**cf fig 10**

$$ Y = \sum_{k=1}^n x_k$$
Chaque $x_k$ suit une loi de Bernouilli (p). Les $X_k$ sont 2 à 2 indépendants. X suit une loi Binomiale (n,p).
$$\forall y \in \{0,1,2, ...,n\} p(Y = y) C_n^y p^y (1-p)^{n-y}$$
$$Y = y_i \Rightarrow y \text{présents} \rightarrow p^y$$
$$n-y \text{absents} \rightarrow (1-p)^{n-y}$$

nb :
$$C_n^y = \frac{n!}{y!(n-y)!}$$
$$A_n^y = \frac{n!}{(n-y)!}$$


Maintenant $n\rightarrow \infty$, alors, comme $np = \lambda =\text{ constante } > 0$.
$$ Y = \sum_{k=1}^n x_k$$
C'est la loi de Poisson, qui ne dépand que de $\lambda$.
$$p(Z=z) = e^{-\lambda} \frac{\lambda^z}{z!} \forall z\in\mathrm{N}$$
$$E(X) = ... = \lambda$$
$$\text{Var} (X) = ... = \lambda$$

### V.A. Réelle vectorielle ###
 

Permet de prendre en compte les éventuelles dépendances entre plusieures variables.

#### Fct de répartition ####
$$F_x (\overrightarrow{x}) = \text{proba} (X_1 \leq x_1, ... , X_n \leq x_m)$$

Propriétés :
$$0 = F_{\overrightarrow x} (-\infty, x_2, ... , x_n) \leq F_{\overrightarrow x} (\overrightarrow x) \leq 1 = F_{\overrightarrow x} (\infty, \infty, ... , \infty)$$
- n = 2
$$\text{proba} (a < x_1 \leq b, c< x_2 \leq d) = F_{x_1, x_2} (b,d) -F_{\overrightarrow x} (b,c) - F_{\overrightarrow x}(a,d) + F_{\overrightarrow x} (a,c)$$

**cf fig 11**

### Densité de probabilité ###
 

$$f_{\overrightarrow x} = \frac{d^n}{d_{x_1},d_{x_2}, ... ,d_{x_n}} F_{\overrightarrow x} (\overrightarrow{x})$$
- n = 2
$$\text{proba} (a < x_1 \leq b, c < x_2 \leq d) = \int\limits_{x_1 \in ]a,b], x_2 \in ]c,d]} f_{\overrightarrow x} \dd x_1, \dd x_2$$

### Lois marginales (n=2) ###
 

couple de v.a. (x, y)
$$F_{x,y} (x,y) \text{connue} \Rightarrow F_x (x) * F_{x,y} (x,\infty) $$
de même
$$f_x (x) = \differential{F_x (x)}{x} = \differential {}{x} F_{x,y} (x, \infty) = \int_\mathrm{R} f_{x,y} (x,y) \dd y$$

### Indépendances - Loi conditionnelle ###
 

Si x et y sont indépendantes alors $\forall (x,y) f_{x,y} (x,y) = f_x(x) \cdot f_y(y)$ et $f_{x,y} (x,y) = f_x(x)\cdot f_y(y)$

Loi conditionnelle $f_{x|y} (x |y ) = \frac{f_{x,y} (x,y)}{f_y(y)}$. Formule des probabilités totales ;
$$F_y(y) = \int\mathrm{R} f_{x,y} (x,y) \dd x = \int\mathrm{R} f_{y|x} (y|x) f_x (x) \dd x$$



## DISCRETE ##

## VECTORIELLE ##


## COURBES DE REGRESSION ##

$\Rightarrow$ minimiser l'errer quadratique moyenne.

On minimise $J = E \{ [y-g(x)^2]\}$

1. Courbes de régression fonction g qui minimise J.

Calcul variationnnel. $\rightarrow$

\begin{align*}
g(x) &= E_{y|x} (Y|X = x) \\
g(x) &= \int_\Re y f_{y|x}(y|x) \mathrm{d}y \\
\text{avec : } f_{y|x} (y|x) = \frac{f_{x,y} (x,y)}{f_x(x)} \\
\end{align*}

Remarque : h qui minimise $E\{[x-h(y)}^2]\}$
$$h(y) = \int_\Re x f_{x|y}(x|y) \mathrm{d}x$$

### Covariance – Corrélation ###

Couple de variables aléatoires (x,y)
Toute fonction h maximable définit une autre v.a. $z = h(x,y)$. En moyenne :
$$E\{h(x,y)\} = \iint_{\Re^2} h(x,y) f_{x,y} (x,y) \mathrm{d}x \mathrm{d}y$$

Moment croisé d'ordre 2 : $h(x,y) = xy \exists (x,y)$.

__Théorème :__ Si X et Y possèdent des moments d'ordre 2, alors E(x,y) existent et vérifie:
$$E^2 (x,y) \leq E(x^2) E(y^2)$$

__Démo partielle :__ Si E(x,y) existe, $E\{(\alpha x +y)\}, \alpha \in \Re$
$$\alpha^2 E(x^2) + 2 \alpha E(x,y) + E(y^2) \geq 0$$

 

$$ 4 E^2(x,y) - 4 E(x^2) E(y^2) \leq 0$$

\begin{align*}
\left . x- E(x) \\
        y- E(y) \right \} v.a centrées \\
E^2(x,y) \leq E(x^2) E(y^2) \\
E^2 ((x-E(x))(y-E(y))) \leq \var(x) \var(y) \\
\end{align*}

Covariance du couple (x,y)

En pratique, on la calcule :
$$ \cov(x,y) = E(x,y) - E(x)E(y)$$
Comme de plus :
\begin{align*}
\cov^2(x,y) \leq \var(x) \var(y) \\
\frac{\cov(x,y)}{\sqrt{\var(x)\var(y)}} = \frac{\cov(x,y)}{\sigma_x \sigma_y}, \in [-y, +y] \\
\equiv \text{coefficiant de corréalation } r \\
\end{align*}

Cas particulier :
Si x et y sont indépendants alors :
\begin{align*}
f_{x,y} (x,y) &= f_x(x) f_y(y) \\
\rightarrow \cov(x,y) &= \iint_{\Re^2} [x-E(x)] [y-E(y)] f_{x,y} (x,y) \dd{x} \dd{y} \\
&= 0\\
\end{align*}

\boxed{__NB :__ Indépendance $\Rightarrow \nRightarrow$ décorrélation}

### Régression linéaire d'un couple de v.a.(x,y)###
 

$f_{x,y}$ comme on cherche $g(x) \aprox y$ sous la forme $g(x) = ax+b$.

Trouver les réels a et b qui minimisent :
\begin{align*}
J = \{[y-ax-b]^2\}
\left { \frac{\delta J}{\delta a} = 0 \\
        \frac{\delta J}{\delta b} = 0 \\
\right .
\frac{\delta J}{\delta a} = E \{f[y-ax-b] (-x)\} = 0
\end{align*}

$$\boxed{\left \{ 0 = E \{[y-ax-b]x\} \\ 0 = E \{[y-ax-b] 1\} \right \}$$

\begin{align*}
\left \{ E(yx) - a E(x^2) -b E(x) &= 0\\
E(y) -a E(x) -b = 0 \right . \\
\begin{bmatrix}  E(x^2) & E(x) \\ E(x) & 1 \end{bmatrix} \begin{bmatrix} a \\ b \end{bmatrix} = \begin{bmatrix} E(xy) \\ E(y) \end{bmatrix}
\end{align*}

det = var(x) que l'on suppose non nul car c'est sans intérêt.

\begin{align*}
a &= \frac{\begin{bmatrix} E(xy) & E(x) \\ E(y) & 1 \end{bmatrix}}{\var x} \\
  &= \frac{E(xy) - E(x)E(y)}{\var (x)} \\
  &= \frac{\cov (x,y)}{\var x} \\
b &= E(y) - aE(x)
\end{align*}

\begin{align*}
y &\equiv ax+b \\
  &= \frac{\cov(x,y)}{\var(x)} (x-E(x)) +E(y) \\
  &= \frac{r \sigma_x \sigma_y}{\sigma_x^2} (x-E(x)) + E(y) \\
  \frac{y-E(y)}{\sigma_y} &\equiv r \frac

  ##########

  Cas particulier :
\begin{align*}
si r &= \pm 1 \\
y &= ax+b
si E\{[y-(ax+b)]^2\} &= 0
y &= ax+b \text{ en moyenne quadratique}
\left \{ E[y-ax+b)] &= 0 \\
\var (y-(ax+b)) = 0 \right .
\end{align*}

### Exemple : ###
 

On cherche x avec un appareil de mesure qui donne $y = x+b$ avec x et B indépendantes. On suppose :
\begin{align*}
B \leadsto& \mathrm{N} (m_B,\sigma_B^2) \\
X \leadsto& \mathrm{N} (o, \sigma_x^2)
\end{align*}

coeff de corrélation y entre x et y ?

\begin{align*}
r &= \frac{\cov (x,y)}{\sigma_x \sigma_y}
E(y) &= E(x+b) \\
     &= E(x) + E(b) \\
     &= m_B \\
\var(y) &= E\{[y-E(y)]^2\} \\
        &= E(y^2) -E^2(y)
        &= E(x^2) + E[(B -m_b)^2] + 2E[x(B -m_B)] \\
        &= \var(x) + \var(B) + \begen{matrix}2E(x) & E (B - m_B) \\ =0 & =0\end{matrix} \\
        &= \sigma_x^2 + \sigma_y^2
\end{align*}

De plus :

\begin{align*}
\cov(x,y) &= E\{ [x - E(x)][y-E(y)]\} \\
          &= E\{ x [x [ x + b - ]]}
\end{align*}













## THEOREMES AUX LIMITES ##

## ESTIMATION PARAMÈTREQUE ##

## TESTS D'HYPETHÉSES ##

## Références ##

BASS : Éléments de calcul des probabilités MASSON
VENTETSEL : Théorie des probabilités MIR
RÉNYI : Calcul des probabilités MASSON (I.GABAY)
JAFFARD : Méthodes de la statistique MASSON
Série SCHAUM
chamilo


